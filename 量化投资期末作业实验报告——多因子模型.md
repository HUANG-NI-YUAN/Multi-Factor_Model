







# 量化投资期末作业实验报告——多因子模型



<center><font size="4">Author’s Name：黄倪远 3200101028</fontfont></center>

<center><font size="4">Date: 2023-2-8<font size="5"></center>

<div style="page-break-after:always"></div>

## Chapter 1： 简介

​	多因子模型目前已成为投资实践的主流方法，并且在衡量与控制风险方面拥有优越性能。本作业要求通过提供的数据构建10-15个因子，并通过线性和机器学习模型进行因子合成，最终回测来检验模型，并分析差距。

​	数据总共有三个层次的数据，分别是财务数据、分析师一致预测数据、日交易数据，其中财务数据为年度数据，分析师一致预测数据、日交易数据为日度数据。

## Chapter 2： 代码思路分析

### 数据读取模块

​	依次读取数据，分别获得财务数据、分析师一致预测数据和日交易数据，并读取交易日数据，自上至下填充自然数生成时间所以，保证每个交易日有唯一一个自然数与之对应。并利用交易日进行数据合并。

​	构建15个因子（具体如下表），对于每个因子，再依次进行去极端值（超过平均离差的n倍的数据）、填充缺失值(用中位数填充)、标准化的过程。最终得到一张因子数据表格。

| 因子                   | 因子指标 | 简介                         |
| ---------------------- | -------- | ---------------------------- |
| 质量                   | ROE      |                              |
| 质量变动               | dROE     |                              |
| 每股收益               | EPS      |                              |
| 每股净资产             | NAPS     |                              |
| 净利润                 | NPT      |                              |
| 净利润增长率           | NPG      |                              |
| 一致预期               | EEPS     |                              |
| 一致预期净利同比       | ENPYOY   |                              |
| 两年预期净利复合增长率 | ENPYOY2  |                              |
| 市值                   | SIZE     | log(SIZE)                    |
| 日收益率               | REV      |                              |
| 换手率                 | VOL      |                              |
| 波动率                 | STD      | （最高价-最低价）/日均交易价 |
| 反转因子               | REV20    | $$ REV_t-REV_{t-20} $$       |
| 反转因子               | VOL20    | $$ VOL_t-VOL_{t-20}$$        |

### 因子合成模块

​	提供三种模型用以回归：线性回归、逻辑回归、XGboost。XGBoost的全称是eXtreme Gradient Boosting，它是经过优化的分布式梯度提升库，旨在高效、灵活且可移植。XGBoost是大规模并行boosting tree的工具，它是目前最快最好的开源 boosting tree工具包，比常见的工具包快10倍以上。XGBoost是由多棵CART(Classification And Regression Tree)，即分类回归树组成，因此他可以处理分类回归等问题。

​	交叉检验部分使用k-folder检验。

​	以日为单位，选取每个交易日表现最好的百分之30%和表现最差的30%作为训练数据。将总样本的60%作为样本内数据来训练，其余当成样本外数据来检验。

## Chapter 3：实验结果

​	由于内存有限，一次性支持9年的数据投入训练和测试，于是以三年为一组进行试验。试验会输出：

1. K-Folder检验的结果
2. 相关性分析：采用平均绝对误差（MAE）和均方误差（MSE）
3. 样本外预测性能：采用准确率（accuracy）和AUC指标
4. 回测策略：返回一张每日的收益率曲线图和年超额收益、波动以及信息率

* 输出示例（以xgboost模型在2016年到2018年的训练结果为例）：

  ```
  # TESTING Model
  Total TimeIndex:[ 1456 , 2186 ]
  Test TimeIndex(in-sample):[ 1456 , 1894 ]
  Predict TimeIndex(out-sample):[ 1895 , 2186 ]
  1.CROSS-VALIDATION:
  [0.76860049 0.76785751 0.76803644 0.76805033 0.76995987]
  2.INTERNAL DEPENDENCE:
  Mean Squared Error(MSE) = 0.156224
  Median Absolute Error(MAE) = 0.241503
  R^2 score = 0.375100
  3.MODOLE EVALUATION:
  training set, accuracy = 0.77
  training set, AUC = 0.85
  cv set, accuracy = 0.77
  cv set, AUC = 0.85
  testing set, time 1895, accuracy = 0.80
  testing set, time 1895, AUC = 0.91
  testing set, time 1896, accuracy = 0.85
  testing set, time 1896, AUC = 0.89
  testing set, time 1897, accuracy = 0.76
  testing set, time 1897, AUC = 0.89
  testing set, time 1898, accuracy = 0.76
  testing set, time 1898, AUC = 0.88
  testing set, time 1899, accuracy = 0.77
  testing set, time 1899, AUC = 0.92
  testing set, time 1900, accuracy = 0.84
  testing set, time 1900, AUC = 0.91
  testing set, time 1901, accuracy = 0.84
  testing set, time 1901, AUC = 0.94
  testing set, time 1902, accuracy = 0.80
  testing set, time 1902, AUC = 0.93
  testing set, time 1903, accuracy = 0.73
  testing set, time 1903, AUC = 0.93
  testing set, time 1904, accuracy = 0.87
  testing set, time 1904, AUC = 0.92
  testing set, time 1905, accuracy = 0.82
  testing set, time 1905, AUC = 0.91
  testing set, time 1906, accuracy = 0.81
  testing set, time 1906, AUC = 0.89
  testing set, time 1907, accuracy = 0.80
  testing set, time 1907, AUC = 0.89
  testing set, time 1908, accuracy = 0.87
  testing set, time 1908, AUC = 0.95
  testing set, time 1909, accuracy = 0.70
  testing set, time 1909, AUC = 0.89
  testing set, time 1910, accuracy = 0.83
  testing set, time 1910, AUC = 0.96
  testing set, time 1911, accuracy = 0.79
  testing set, time 1911, AUC = 0.88
  testing set, time 1912, accuracy = 0.79
  testing set, time 1912, AUC = 0.89
  testing set, time 1913, accuracy = 0.63
  testing set, time 1913, AUC = 0.93
  testing set, time 1914, accuracy = 0.91
  testing set, time 1914, AUC = 0.97
  testing set, time 1915, accuracy = 0.82
  testing set, time 1915, AUC = 0.92
  testing set, time 1916, accuracy = 0.85
  testing set, time 1916, AUC = 0.96
  testing set, time 1917, accuracy = 0.65
  testing set, time 1917, AUC = 0.91
  testing set, time 1918, accuracy = 0.82
  testing set, time 1918, AUC = 0.92
  testing set, time 1919, accuracy = 0.67
  testing set, time 1919, AUC = 0.76
  testing set, time 1920, accuracy = 0.82
  testing set, time 1920, AUC = 0.89
  testing set, time 1921, accuracy = 0.87
  testing set, time 1921, AUC = 0.94
  testing set, time 1922, accuracy = 0.76
  testing set, time 1922, AUC = 0.86
  testing set, time 1923, accuracy = 0.76
  testing set, time 1923, AUC = 0.88
  testing set, time 1924, accuracy = 0.69
  testing set, time 1924, AUC = 0.94
  testing set, time 1925, accuracy = 0.70
  testing set, time 1925, AUC = 0.95
  testing set, time 1926, accuracy = 0.84
  testing set, time 1926, AUC = 0.90
  testing set, time 1927, accuracy = 0.82
  testing set, time 1927, AUC = 0.93
  testing set, time 1928, accuracy = 0.77
  testing set, time 1928, AUC = 0.85
  testing set, time 1929, accuracy = 0.75
  testing set, time 1929, AUC = 0.85
  testing set, time 1930, accuracy = 0.70
  testing set, time 1930, AUC = 0.83
  testing set, time 1931, accuracy = 0.77
  testing set, time 1931, AUC = 0.87
  testing set, time 1932, accuracy = 0.79
  testing set, time 1932, AUC = 0.88
  testing set, time 1933, accuracy = 0.62
  testing set, time 1933, AUC = 0.73
  testing set, time 1934, accuracy = 0.77
  testing set, time 1934, AUC = 0.89
  testing set, time 1935, accuracy = 0.79
  testing set, time 1935, AUC = 0.86
  testing set, time 1936, accuracy = 0.74
  testing set, time 1936, AUC = 0.90
  testing set, time 1937, accuracy = 0.66
  testing set, time 1937, AUC = 0.82
  testing set, time 1938, accuracy = 0.76
  testing set, time 1938, AUC = 0.87
  testing set, time 1939, accuracy = 0.76
  testing set, time 1939, AUC = 0.88
  testing set, time 1940, accuracy = 0.87
  testing set, time 1940, AUC = 0.92
  testing set, time 1941, accuracy = 0.78
  testing set, time 1941, AUC = 0.87
  testing set, time 1942, accuracy = 0.82
  testing set, time 1942, AUC = 0.93
  testing set, time 1943, accuracy = 0.84
  testing set, time 1943, AUC = 0.90
  testing set, time 1944, accuracy = 0.71
  testing set, time 1944, AUC = 0.87
  testing set, time 1945, accuracy = 0.67
  testing set, time 1945, AUC = 0.84
  testing set, time 1946, accuracy = 0.77
  testing set, time 1946, AUC = 0.90
  testing set, time 1947, accuracy = 0.83
  testing set, time 1947, AUC = 0.91
  testing set, time 1948, accuracy = 0.84
  testing set, time 1948, AUC = 0.95
  testing set, time 1949, accuracy = 0.80
  testing set, time 1949, AUC = 0.94
  testing set, time 1950, accuracy = 0.72
  testing set, time 1950, AUC = 0.85
  testing set, time 1951, accuracy = 0.88
  testing set, time 1951, AUC = 0.96
  testing set, time 1952, accuracy = 0.85
  testing set, time 1952, AUC = 0.94
  testing set, time 1953, accuracy = 0.76
  testing set, time 1953, AUC = 0.96
  testing set, time 1954, accuracy = 0.81
  testing set, time 1954, AUC = 0.94
  testing set, time 1955, accuracy = 0.92
  testing set, time 1955, AUC = 0.97
  testing set, time 1956, accuracy = 0.75
  testing set, time 1956, AUC = 0.89
  testing set, time 1957, accuracy = 0.84
  testing set, time 1957, AUC = 0.93
  testing set, time 1958, accuracy = 0.85
  testing set, time 1958, AUC = 0.95
  testing set, time 1959, accuracy = 0.69
  testing set, time 1959, AUC = 0.84
  testing set, time 1960, accuracy = 0.83
  testing set, time 1960, AUC = 0.91
  testing set, time 1961, accuracy = 0.80
  testing set, time 1961, AUC = 0.89
  testing set, time 1962, accuracy = 0.81
  testing set, time 1962, AUC = 0.92
  testing set, time 1963, accuracy = 0.73
  testing set, time 1963, AUC = 0.95
  testing set, time 1964, accuracy = 0.78
  testing set, time 1964, AUC = 0.93
  testing set, time 1965, accuracy = 0.70
  testing set, time 1965, AUC = 0.97
  testing set, time 1966, accuracy = 0.70
  testing set, time 1966, AUC = 0.97
  testing set, time 1967, accuracy = 0.89
  testing set, time 1967, AUC = 0.96
  testing set, time 1968, accuracy = 0.87
  testing set, time 1968, AUC = 0.95
  testing set, time 1969, accuracy = 0.65
  testing set, time 1969, AUC = 0.95
  testing set, time 1970, accuracy = 0.75
  testing set, time 1970, AUC = 0.89
  testing set, time 1971, accuracy = 0.86
  testing set, time 1971, AUC = 0.92
  testing set, time 1972, accuracy = 0.74
  testing set, time 1972, AUC = 0.95
  testing set, time 1973, accuracy = 0.65
  testing set, time 1973, AUC = 0.85
  testing set, time 1974, accuracy = 0.78
  testing set, time 1974, AUC = 0.90
  testing set, time 1975, accuracy = 0.80
  testing set, time 1975, AUC = 0.88
  testing set, time 1976, accuracy = 0.76
  testing set, time 1976, AUC = 0.88
  testing set, time 1977, accuracy = 0.82
  testing set, time 1977, AUC = 0.90
  testing set, time 1978, accuracy = 0.79
  testing set, time 1978, AUC = 0.92
  testing set, time 1979, accuracy = 0.91
  testing set, time 1979, AUC = 0.96
  testing set, time 1980, accuracy = 0.84
  testing set, time 1980, AUC = 0.92
  testing set, time 1981, accuracy = 0.85
  testing set, time 1981, AUC = 0.96
  testing set, time 1982, accuracy = 0.82
  testing set, time 1982, AUC = 0.92
  testing set, time 1983, accuracy = 0.72
  testing set, time 1983, AUC = 0.89
  testing set, time 1984, accuracy = 0.83
  testing set, time 1984, AUC = 0.92
  testing set, time 1985, accuracy = 0.63
  testing set, time 1985, AUC = 0.76
  testing set, time 1986, accuracy = 0.64
  testing set, time 1986, AUC = 0.86
  testing set, time 1987, accuracy = 0.80
  testing set, time 1987, AUC = 0.94
  testing set, time 1988, accuracy = 0.76
  testing set, time 1988, AUC = 0.90
  testing set, time 1989, accuracy = 0.54
  testing set, time 1989, AUC = 0.69
  testing set, time 1990, accuracy = 0.69
  testing set, time 1990, AUC = 0.91
  testing set, time 1991, accuracy = 0.72
  testing set, time 1991, AUC = 0.92
  testing set, time 1992, accuracy = 0.64
  testing set, time 1992, AUC = 0.82
  testing set, time 1993, accuracy = 0.71
  testing set, time 1993, AUC = 0.88
  testing set, time 1994, accuracy = 0.88
  testing set, time 1994, AUC = 0.95
  testing set, time 1995, accuracy = 0.80
  testing set, time 1995, AUC = 0.90
  testing set, time 1996, accuracy = 0.80
  testing set, time 1996, AUC = 0.94
  testing set, time 1997, accuracy = 0.63
  testing set, time 1997, AUC = 0.96
  testing set, time 1998, accuracy = 0.89
  testing set, time 1998, AUC = 0.93
  testing set, time 1999, accuracy = 0.69
  testing set, time 1999, AUC = 0.90
  testing set, time 2000, accuracy = 0.88
  testing set, time 2000, AUC = 0.95
  testing set, time 2001, accuracy = 0.88
  testing set, time 2001, AUC = 0.92
  testing set, time 2002, accuracy = 0.84
  testing set, time 2002, AUC = 0.95
  testing set, time 2003, accuracy = 0.86
  testing set, time 2003, AUC = 0.93
  testing set, time 2004, accuracy = 0.78
  testing set, time 2004, AUC = 0.96
  testing set, time 2005, accuracy = 0.84
  testing set, time 2005, AUC = 0.93
  testing set, time 2006, accuracy = 0.89
  testing set, time 2006, AUC = 0.96
  testing set, time 2007, accuracy = 0.87
  testing set, time 2007, AUC = 0.97
  testing set, time 2008, accuracy = 0.86
  testing set, time 2008, AUC = 0.94
  testing set, time 2009, accuracy = 0.82
  testing set, time 2009, AUC = 0.92
  testing set, time 2010, accuracy = 0.80
  testing set, time 2010, AUC = 0.89
  testing set, time 2011, accuracy = 0.80
  testing set, time 2011, AUC = 0.89
  testing set, time 2012, accuracy = 0.74
  testing set, time 2012, AUC = 0.95
  testing set, time 2013, accuracy = 0.85
  testing set, time 2013, AUC = 0.93
  testing set, time 2014, accuracy = 0.85
  testing set, time 2014, AUC = 0.93
  testing set, time 2015, accuracy = 0.72
  testing set, time 2015, AUC = 0.92
  testing set, time 2016, accuracy = 0.82
  testing set, time 2016, AUC = 0.92
  testing set, time 2017, accuracy = 0.63
  testing set, time 2017, AUC = 0.92
  testing set, time 2018, accuracy = 0.77
  testing set, time 2018, AUC = 0.87
  testing set, time 2019, accuracy = 0.58
  testing set, time 2019, AUC = 0.84
  testing set, time 2020, accuracy = 0.79
  testing set, time 2020, AUC = 0.87
  testing set, time 2021, accuracy = 0.81
  testing set, time 2021, AUC = 0.94
  testing set, time 2022, accuracy = 0.85
  testing set, time 2022, AUC = 0.90
  testing set, time 2023, accuracy = 0.83
  testing set, time 2023, AUC = 0.91
  testing set, time 2024, accuracy = 0.67
  testing set, time 2024, AUC = 0.83
  testing set, time 2025, accuracy = 0.74
  testing set, time 2025, AUC = 0.85
  testing set, time 2026, accuracy = 0.79
  testing set, time 2026, AUC = 0.88
  testing set, time 2027, accuracy = 0.78
  testing set, time 2027, AUC = 0.88
  testing set, time 2028, accuracy = 0.71
  testing set, time 2028, AUC = 0.92
  testing set, time 2029, accuracy = 0.82
  testing set, time 2029, AUC = 0.91
  testing set, time 2030, accuracy = 0.81
  testing set, time 2030, AUC = 0.90
  testing set, time 2031, accuracy = 0.76
  testing set, time 2031, AUC = 0.87
  testing set, time 2032, accuracy = 0.71
  testing set, time 2032, AUC = 0.83
  testing set, time 2033, accuracy = 0.78
  testing set, time 2033, AUC = 0.88
  testing set, time 2034, accuracy = 0.83
  testing set, time 2034, AUC = 0.90
  testing set, time 2035, accuracy = 0.65
  testing set, time 2035, AUC = 0.85
  testing set, time 2036, accuracy = 0.75
  testing set, time 2036, AUC = 0.90
  testing set, time 2037, accuracy = 0.67
  testing set, time 2037, AUC = 0.85
  testing set, time 2038, accuracy = 0.80
  testing set, time 2038, AUC = 0.92
  testing set, time 2039, accuracy = 0.77
  testing set, time 2039, AUC = 0.88
  testing set, time 2040, accuracy = 0.76
  testing set, time 2040, AUC = 0.90
  testing set, time 2041, accuracy = 0.67
  testing set, time 2041, AUC = 0.90
  testing set, time 2042, accuracy = 0.82
  testing set, time 2042, AUC = 0.90
  testing set, time 2043, accuracy = 0.81
  testing set, time 2043, AUC = 0.92
  testing set, time 2044, accuracy = 0.72
  testing set, time 2044, AUC = 0.94
  testing set, time 2045, accuracy = 0.87
  testing set, time 2045, AUC = 0.92
  testing set, time 2046, accuracy = 0.85
  testing set, time 2046, AUC = 0.92
  testing set, time 2047, accuracy = 0.73
  testing set, time 2047, AUC = 0.92
  testing set, time 2048, accuracy = 0.78
  testing set, time 2048, AUC = 0.90
  testing set, time 2049, accuracy = 0.81
  testing set, time 2049, AUC = 0.90
  testing set, time 2050, accuracy = 0.84
  testing set, time 2050, AUC = 0.90
  testing set, time 2051, accuracy = 0.75
  testing set, time 2051, AUC = 0.91
  testing set, time 2052, accuracy = 0.81
  testing set, time 2052, AUC = 0.90
  testing set, time 2053, accuracy = 0.68
  testing set, time 2053, AUC = 0.93
  testing set, time 2054, accuracy = 0.57
  testing set, time 2054, AUC = 0.95
  testing set, time 2055, accuracy = 0.86
  testing set, time 2055, AUC = 0.94
  testing set, time 2056, accuracy = 0.84
  testing set, time 2056, AUC = 0.97
  testing set, time 2057, accuracy = 0.75
  testing set, time 2057, AUC = 0.92
  testing set, time 2058, accuracy = 0.76
  testing set, time 2058, AUC = 0.87
  testing set, time 2059, accuracy = 0.80
  testing set, time 2059, AUC = 0.93
  testing set, time 2060, accuracy = 0.77
  testing set, time 2060, AUC = 0.86
  testing set, time 2061, accuracy = 0.59
  testing set, time 2061, AUC = 0.68
  testing set, time 2062, accuracy = 0.77
  testing set, time 2062, AUC = 0.91
  testing set, time 2063, accuracy = 0.76
  testing set, time 2063, AUC = 0.89
  testing set, time 2064, accuracy = 0.80
  testing set, time 2064, AUC = 0.93
  testing set, time 2065, accuracy = 0.64
  testing set, time 2065, AUC = 0.93
  testing set, time 2066, accuracy = 0.67
  testing set, time 2066, AUC = 0.93
  testing set, time 2067, accuracy = 0.75
  testing set, time 2067, AUC = 0.90
  testing set, time 2068, accuracy = 0.66
  testing set, time 2068, AUC = 0.84
  testing set, time 2069, accuracy = 0.77
  testing set, time 2069, AUC = 0.89
  testing set, time 2070, accuracy = 0.65
  testing set, time 2070, AUC = 0.89
  testing set, time 2071, accuracy = 0.62
  testing set, time 2071, AUC = 0.84
  testing set, time 2072, accuracy = 0.84
  testing set, time 2072, AUC = 0.93
  testing set, time 2073, accuracy = 0.70
  testing set, time 2073, AUC = 0.85
  testing set, time 2074, accuracy = 0.56
  testing set, time 2074, AUC = 0.83
  testing set, time 2075, accuracy = 0.73
  testing set, time 2075, AUC = 0.86
  testing set, time 2076, accuracy = 0.59
  testing set, time 2076, AUC = 0.70
  testing set, time 2077, accuracy = 0.80
  testing set, time 2077, AUC = 0.86
  testing set, time 2078, accuracy = 0.88
  testing set, time 2078, AUC = 0.96
  testing set, time 2079, accuracy = 0.86
  testing set, time 2079, AUC = 0.94
  testing set, time 2080, accuracy = 0.83
  testing set, time 2080, AUC = 0.91
  testing set, time 2081, accuracy = 0.85
  testing set, time 2081, AUC = 0.95
  testing set, time 2082, accuracy = 0.64
  testing set, time 2082, AUC = 0.86
  testing set, time 2083, accuracy = 0.80
  testing set, time 2083, AUC = 0.91
  testing set, time 2084, accuracy = 0.73
  testing set, time 2084, AUC = 0.89
  testing set, time 2085, accuracy = 0.70
  testing set, time 2085, AUC = 0.81
  testing set, time 2086, accuracy = 0.73
  testing set, time 2086, AUC = 0.84
  testing set, time 2087, accuracy = 0.75
  testing set, time 2087, AUC = 0.92
  testing set, time 2088, accuracy = 0.59
  testing set, time 2088, AUC = 0.87
  testing set, time 2089, accuracy = 0.69
  testing set, time 2089, AUC = 0.86
  testing set, time 2090, accuracy = 0.69
  testing set, time 2090, AUC = 0.84
  testing set, time 2091, accuracy = 0.85
  testing set, time 2091, AUC = 0.91
  testing set, time 2092, accuracy = 0.83
  testing set, time 2092, AUC = 0.90
  testing set, time 2093, accuracy = 0.83
  testing set, time 2093, AUC = 0.91
  testing set, time 2094, accuracy = 0.83
  testing set, time 2094, AUC = 0.91
  testing set, time 2095, accuracy = 0.66
  testing set, time 2095, AUC = 0.81
  testing set, time 2096, accuracy = 0.75
  testing set, time 2096, AUC = 0.88
  testing set, time 2097, accuracy = 0.61
  testing set, time 2097, AUC = 0.87
  testing set, time 2098, accuracy = 0.80
  testing set, time 2098, AUC = 0.88
  testing set, time 2099, accuracy = 0.82
  testing set, time 2099, AUC = 0.88
  testing set, time 2100, accuracy = 0.73
  testing set, time 2100, AUC = 0.89
  testing set, time 2101, accuracy = 0.82
  testing set, time 2101, AUC = 0.89
  testing set, time 2102, accuracy = 0.78
  testing set, time 2102, AUC = 0.88
  testing set, time 2103, accuracy = 0.63
  testing set, time 2103, AUC = 0.84
  testing set, time 2104, accuracy = 0.81
  testing set, time 2104, AUC = 0.90
  testing set, time 2105, accuracy = 0.70
  testing set, time 2105, AUC = 0.83
  testing set, time 2106, accuracy = 0.62
  testing set, time 2106, AUC = 0.70
  testing set, time 2107, accuracy = 0.73
  testing set, time 2107, AUC = 0.84
  testing set, time 2108, accuracy = 0.71
  testing set, time 2108, AUC = 0.82
  testing set, time 2109, accuracy = 0.72
  testing set, time 2109, AUC = 0.84
  testing set, time 2110, accuracy = 0.70
  testing set, time 2110, AUC = 0.80
  testing set, time 2111, accuracy = 0.63
  testing set, time 2111, AUC = 0.84
  testing set, time 2112, accuracy = 0.85
  testing set, time 2112, AUC = 0.93
  testing set, time 2113, accuracy = 0.74
  testing set, time 2113, AUC = 0.94
  testing set, time 2114, accuracy = 0.87
  testing set, time 2114, AUC = 0.93
  testing set, time 2115, accuracy = 0.65
  testing set, time 2115, AUC = 0.83
  testing set, time 2116, accuracy = 0.70
  testing set, time 2116, AUC = 0.82
  testing set, time 2117, accuracy = 0.71
  testing set, time 2117, AUC = 0.83
  testing set, time 2118, accuracy = 0.68
  testing set, time 2118, AUC = 0.88
  testing set, time 2119, accuracy = 0.83
  testing set, time 2119, AUC = 0.92
  testing set, time 2120, accuracy = 0.66
  testing set, time 2120, AUC = 0.88
  testing set, time 2121, accuracy = 0.79
  testing set, time 2121, AUC = 0.93
  testing set, time 2122, accuracy = 0.74
  testing set, time 2122, AUC = 0.90
  testing set, time 2123, accuracy = 0.64
  testing set, time 2123, AUC = 0.90
  testing set, time 2124, accuracy = 0.83
  testing set, time 2124, AUC = 0.91
  testing set, time 2125, accuracy = 0.77
  testing set, time 2125, AUC = 0.94
  testing set, time 2126, accuracy = 0.66
  testing set, time 2126, AUC = 0.83
  testing set, time 2127, accuracy = 0.67
  testing set, time 2127, AUC = 0.84
  testing set, time 2128, accuracy = 0.79
  testing set, time 2128, AUC = 0.88
  testing set, time 2129, accuracy = 0.82
  testing set, time 2129, AUC = 0.94
  testing set, time 2130, accuracy = 0.60
  testing set, time 2130, AUC = 0.91
  testing set, time 2131, accuracy = 0.95
  testing set, time 2131, AUC = 0.99
  testing set, time 2132, accuracy = 0.81
  testing set, time 2132, AUC = 0.95
  testing set, time 2133, accuracy = 0.81
  testing set, time 2133, AUC = 0.89
  testing set, time 2134, accuracy = 0.82
  testing set, time 2134, AUC = 0.91
  testing set, time 2135, accuracy = 0.68
  testing set, time 2135, AUC = 0.91
  testing set, time 2136, accuracy = 0.75
  testing set, time 2136, AUC = 0.94
  testing set, time 2137, accuracy = 0.54
  testing set, time 2137, AUC = 0.82
  testing set, time 2138, accuracy = 0.78
  testing set, time 2138, AUC = 0.89
  testing set, time 2139, accuracy = 0.75
  testing set, time 2139, AUC = 0.92
  testing set, time 2140, accuracy = 0.86
  testing set, time 2140, AUC = 0.97
  testing set, time 2141, accuracy = 0.84
  testing set, time 2141, AUC = 0.94
  testing set, time 2142, accuracy = 0.67
  testing set, time 2142, AUC = 0.91
  testing set, time 2143, accuracy = 0.76
  testing set, time 2143, AUC = 0.94
  testing set, time 2144, accuracy = 0.80
  testing set, time 2144, AUC = 0.94
  testing set, time 2145, accuracy = 0.68
  testing set, time 2145, AUC = 0.87
  testing set, time 2146, accuracy = 0.69
  testing set, time 2146, AUC = 0.92
  testing set, time 2147, accuracy = 0.55
  testing set, time 2147, AUC = 0.81
  testing set, time 2148, accuracy = 0.87
  testing set, time 2148, AUC = 0.95
  testing set, time 2149, accuracy = 0.83
  testing set, time 2149, AUC = 0.92
  testing set, time 2150, accuracy = 0.54
  testing set, time 2150, AUC = 0.78
  testing set, time 2151, accuracy = 0.75
  testing set, time 2151, AUC = 0.84
  testing set, time 2152, accuracy = 0.59
  testing set, time 2152, AUC = 0.92
  testing set, time 2153, accuracy = 0.62
  testing set, time 2153, AUC = 0.95
  testing set, time 2154, accuracy = 0.78
  testing set, time 2154, AUC = 0.88
  testing set, time 2155, accuracy = 0.56
  testing set, time 2155, AUC = 0.89
  testing set, time 2156, accuracy = 0.83
  testing set, time 2156, AUC = 0.93
  testing set, time 2157, accuracy = 0.63
  testing set, time 2157, AUC = 0.81
  testing set, time 2158, accuracy = 0.80
  testing set, time 2158, AUC = 0.91
  testing set, time 2159, accuracy = 0.84
  testing set, time 2159, AUC = 0.94
  testing set, time 2160, accuracy = 0.78
  testing set, time 2160, AUC = 0.89
  testing set, time 2161, accuracy = 0.62
  testing set, time 2161, AUC = 0.90
  testing set, time 2162, accuracy = 0.80
  testing set, time 2162, AUC = 0.92
  testing set, time 2163, accuracy = 0.83
  testing set, time 2163, AUC = 0.92
  testing set, time 2164, accuracy = 0.88
  testing set, time 2164, AUC = 0.94
  testing set, time 2165, accuracy = 0.66
  testing set, time 2165, AUC = 0.91
  testing set, time 2166, accuracy = 0.74
  testing set, time 2166, AUC = 0.92
  testing set, time 2167, accuracy = 0.62
  testing set, time 2167, AUC = 0.80
  testing set, time 2168, accuracy = 0.75
  testing set, time 2168, AUC = 0.88
  testing set, time 2169, accuracy = 0.85
  testing set, time 2169, AUC = 0.93
  testing set, time 2170, accuracy = 0.76
  testing set, time 2170, AUC = 0.89
  testing set, time 2171, accuracy = 0.82
  testing set, time 2171, AUC = 0.90
  testing set, time 2172, accuracy = 0.61
  testing set, time 2172, AUC = 0.83
  testing set, time 2173, accuracy = 0.80
  testing set, time 2173, AUC = 0.88
  testing set, time 2174, accuracy = 0.79
  testing set, time 2174, AUC = 0.86
  testing set, time 2175, accuracy = 0.77
  testing set, time 2175, AUC = 0.87
  testing set, time 2176, accuracy = 0.61
  testing set, time 2176, AUC = 0.83
  testing set, time 2177, accuracy = 0.79
  testing set, time 2177, AUC = 0.87
  testing set, time 2178, accuracy = 0.62
  testing set, time 2178, AUC = 0.73
  testing set, time 2179, accuracy = 0.65
  testing set, time 2179, AUC = 0.86
  testing set, time 2180, accuracy = 0.82
  testing set, time 2180, AUC = 0.90
  testing set, time 2181, accuracy = 0.56
  testing set, time 2181, AUC = 0.60
  testing set, time 2182, accuracy = 0.79
  testing set, time 2182, AUC = 0.91
  testing set, time 2183, accuracy = 0.77
  testing set, time 2183, AUC = 0.92
  testing set, time 2184, accuracy = 0.79
  testing set, time 2184, AUC = 0.93
  testing set, time 2185, accuracy = 0.67
  testing set, time 2185, AUC = 0.76
  testing set, time 2186, accuracy = 0.84
  testing set, time 2186, AUC = 0.93
  4.Back Testing:
  annual excess return = 27.53
  annual excess volatility = 0.22
  information ratio = 124.90
  
  ```

  ![xgboost_1456_2186](C:\Users\HUANG\Desktop\3200101028_黄倪远_多因子模型\Result\xgboost_1456_2186.png)

​	发现日收益率整体为正，而且随着时间的推移，预测效果并没有明显下降。

​	整合三种模型在各个时间段上的训练结果如下：

| method  | begin | end  | annual excess return | annual excess volatility | information ratio |
| ------- | ----- | ---- | -------------------- | ------------------------ | ----------------- |
| xgboost | 0     | 728  | 23.4                 | 0.3                      | 76.76             |
| xgboost | 242   | 966  | 26.21                | 0.28                     | 94.9              |
| xgboost | 486   | 1211 | 27.08                | 0.24                     | 113.9             |
| xgboost | 729   | 1455 | 30.98                | 0.31                     | 98.75             |
| xgboost | 967   | 1699 | 32.06                | 0.28                     | 116.02            |
| xgboost | 1212  | 1943 | 31.01                | 0.21                     | 145.21            |
| xgboost | 1456  | 2186 | 27.53                | 0.22                     | 124.9             |
| LR      | 0     | 728  | 22.4                 | 0.33                     | 67.36             |
| LR      | 242   | 966  | 25.5                 | 0.32                     | 80.68             |
| LR      | 486   | 1211 | 27.4                 | 0.28                     | 96.35             |
| LR      | 729   | 1455 | 32.82                | 0.35                     | 93.91             |
| LR      | 967   | 1699 | 30.94                | 0.27                     | 114.03            |
| LR      | 1212  | 1943 | 28.49                | 0.24                     | 117.42            |
| LR      | 1456  | 2186 | 26.75                | 0.26                     | 101.57            |
| LOGI    | 0     | 728  | 23.18                | 0.36                     | 64.38             |
| LOGI    | 242   | 966  | 26.04                | 0.33                     | 77.91             |
| LOGI    | 486   | 1211 | 27.42                | 0.27                     | 102.3             |
| LOGI    | 729   | 1455 | 32.47                | 0.35                     | 92.06             |
| LOGI    | 967   | 1699 | 30.65                | 0.27                     | 115.38            |
| LOGI    | 1212  | 1943 | 27.87                | 0.21                     | 130.9             |
| LOGI    | 1456  | 2186 | 27.45                | 0.24                     | 116.29            |

​	首先，该超额收益率的计算为制定策略为，每个交易日根据回归的模型，购买前50只表现最好的股票。将每日的收益率累乘得到的超收益率，显然，这没有考虑交易费用和每天的无风险手收益率，所以超额收益率的指标大得有点不真实。不过依然可以作为模型之间的比较指标。

​	仅从该数据看，依据机器学习模型和线性模型的策略在超额收益上并没有太大的差别，基于机器学习的策略略微比基于线性模型的策略稳定一点。

​	完整的实验结果见“Result”文件夹。

## Chapter 4：实验总结

​	代码基本实现作业要求，按照要求完成了15个因子的构建，并尝试了三种模型进行组合。但仍有一些点因暂时没想到更好的办法，期待后续继续改进。

1. 数据方面，一些动量因子的构建有些不严谨，将数据平移之后相减会造成一只股票的第一个时间上的数据没有意义，在代码上没有考虑。
2. 没有考虑股票停牌和上市的情况，将所有数据读入处理。
3. 因子构建上，15个因子构建的并不全，大多数因子比较简单，多为直接读取或者差分，可以利用主成分分析等方法进行特征因子的提取。
4. 策略构建上简单的等权重购买50只表现最好的股票，可以考虑更复杂的策略模型，如加权或者做多和做空相结合等等。
5. 回测指标的构件上有些单调和不严谨，如前文所说，构建超额收益率的时候没有考虑无风险收益。此外可以加入更多指标来衡量多因子模型。

<div style="page-break-after:always"></div>

## Chapter 5：附录——源码（python）

```python
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
# @Time : 2023/2/8 15:26
# @Author : Huang Niyuan
# @Email : 3123824465@qq.com
# @File : Code_Mul.py
# @Software: PyCharm

import datetime
import numpy as np
import pandas as pd
import math


# 0. 设置基础参数
class Para():
    # - 文件路径
    path_tradingday = '../Data/data/tradingdays/tradingdays.csv'
    path_financial = '../Data/data/financial/financial_'
    path_dailyquote = '../Data/data/daily_quote/daily_quote_'
    path_analyst = '../Data/data/analyst/analyst_'

    # - 数据范围
    begin_year = 2016
    end_year = 2018

    # - 极端值判断范围
    n_extreme = 20

    # - 训练集和测试集
    begin_timeindex = 0
    end_timeindex = 500
    in_sample_rate = 0.6
    time_in_sample = range(begin_timeindex, begin_timeindex+int((end_timeindex - begin_timeindex) * in_sample_rate) + 1)
    time_test = range(begin_timeindex+int((end_timeindex - begin_timeindex) * in_sample_rate) + 1, end_timeindex + 1)

    # - 交叉检验参数
    evaluation_metrics = 'cross_val_score'  # 'cross_val_score' 'cross_validate'
    cv = 5  # k value in k-folder
    percent_cv = 0.1  # -- percentage of cross validation samples

    # - 模型参数
    method = 'XGBOOST-C'  # 'LOGI' 'XGBOOST-C' 'LR'
    percent_select = [0.3, 0.3]  # -- 30% positive samples, 30% negative samples

    seed = 42  # -- random seed

    # - logistic模型参数
    logi_c = 0.0006  # -- logistic regression parameter

    # - xgboost模型参数
    xgbc_n_estimators = 100  # -- xgboost classifier parameter
    xgbc_learning_rate = 0.1  # -- xgboost classifier parameter
    xgbc_subsample_C = 0.95  # -- xgboost classifier parameter
    xgbc_max_depth = 3  # -- xgboost classifier parameter


# 1. 读入数据
# - 1.1 读取财务报表、日交易、分析师一致预测的数据
def ReadData(doc, begin, end):
    """
    doc: 读取数据的地址，str
    begin: 开始年份, int
    end: 结束年份, int
    """
    df = pd.DataFrame()
    print("begin reading!")
    for year in range(begin, end + 1):
        path = doc + str(year) + ".csv"
        print(path)
        data = pd.read_csv(path)
        data['year'] = year
        df = pd.concat([df, data], ignore_index=True)

    print("end reading!")

    return df


# - 1.2 读取交易日数据
def ReadTradingDay(para):
    """
    TradingDay：交易日期, datetime
    year: TradingDay的年， int
    month：TradingDay的月， int
    day： TradingDay的日， int
    """
    df = pd.read_csv(para.path_tradingday)
    df['TradingDay'] = pd.to_datetime(df['TradingDay'])
    df['year'] = df['TradingDay'].dt.year
    df['month'] = df['TradingDay'].dt.month
    df['day'] = df['TradingDay'].dt.day
    df['TimeIndex'] = range(len(df))

    return df


# 2. 生成因子数据
# - 2.1 基于财务数据的因子数据
def Financial_Factor(df):
    """
    df:财务数据
    SecuCode：股票代码, int
    year：报表年份, int
    InfoPublDate: 公告时间， datetime
    ROE: Return on Equity，净资产收益率，质量因子
    dROE: 净资产收益率变动
    EPS: 每股收益
    dEPS: 每股收益变动
    NAPS: 每股净资产
    DP: 应付股利
    NPT: 净利润
    NPG: 净利润增长率
    """
    df = df.drop_duplicates(['SecuCode', 'year'])  # 对于同一个股票同一年份的数据保留其中一个
    df = df.sort_values(by=['SecuCode', 'year'])
    # col = df.pop('year')
    # df.insert(loc=1, column = 'year', value = col)

    new_df = pd.DataFrame()
    new_df['SecuCode'] = df['SecuCode']
    new_df['year'] = df['year']
    new_df['ROE'] = df['ROE']
    new_df['dROE'] = df['ROE'].diff()
    # new_df.loc[df['year'] == 2010, 'dROE'] = new_df.loc[df['year'] == 2011, 'dROE']
    new_df['EPS'] = df['EPS']
    # new_df['dEPS'] = df['EPS'].diff()
    # new_df.loc[df['year']==2010, 'dEPS'] = new_df.loc[df['year']==2011, 'dEPS']
    new_df['NAPS'] = df['NAPS']
    # new_df['DP'] = df['DividendPayable']
    new_df['NPT'] = df['NetProfit']
    new_df['NPG'] = df['NetProfit'].diff() / df['NetProfit']

    return new_df


# - 2.2 基于分析师数据的因子数据
def Analyst_Factor(df):
    """
    EEPS：一致预期
    ENPYOY：一致预期净利同比
    ENPYOY2：两年预期净利复合增长率
    """
    new_df = pd.DataFrame()
    new_df['SecuCode'] = df['SecuCode']
    new_df['TradingDay'] = pd.to_datetime(df['TradingDay'])
    new_df['year'] = new_df['TradingDay'].dt.year
    new_df['month'] = new_df['TradingDay'].dt.month
    new_df['day'] = new_df['TradingDay'].dt.day
    new_df['EEPS'] = df['eps_est1']
    new_df['ENPYOY'] = df['npyoy_est']
    new_df['ENPYOY2'] = df['npyoy2_est']
    new_df = new_df.sort_values(by=['SecuCode', 'TradingDay'])

    return new_df


# - 2.3 基于日交易数据的因子数据
def Daily_Quote_Factor(df):
    """
    SecuCode：股票代码, int
    TradingDay：交易日期, datetime
    year: TradingDay的年， int
    month：TradingDay的月， int
    day： TradingDay的日， int
    Size：市值
    REV：日收益率
    REV20：反转因子，REV_t-REV_(t-20)
    VOL：换手率
    VOL20：反转因子，VOL_t-VOL_(t-20)
    STD：波动率，（最高价-最低价）/日均交易价
    """
    new_df = pd.DataFrame()
    new_df['SecuCode'] = df['SecuCode']
    new_df['TradingDay'] = pd.to_datetime(df['TradingDay'])
    new_df['year'] = new_df['TradingDay'].dt.year
    new_df['month'] = new_df['TradingDay'].dt.month
    new_df['day'] = new_df['TradingDay'].dt.day
    new_df['Size'] = df['Ashares'] * df['TurnoverValue'] / df['TradingVolumes']
    new_df['Size'] = new_df['Size'].apply(np.log)
    new_df['REV'] = df['ret']
    new_df['VOL'] = df['TradingVolumes'] / df['AFloats']
    new_df['STD'] = (df['HighPrice'] - df['LowPrice']) / (df['TurnoverValue'] / df['TradingVolumes'])
    new_df = new_df.sort_values(by=['SecuCode', 'TradingDay'])
    new_df['REV20'] = new_df['REV'] - new_df['REV'].shift(20)
    new_df['VOL20'] = new_df['VOL'] - new_df['VOL'].shift(20)

    return new_df


# - 2.4 合并因子数据
def MergeFactors(para, df_f, df_a, df_dq):
    """
    df_f: 财务数据，年度数据
    df_a: 分析师一致预测数据，日度数据
    df_dq: 日交易数据， 日度数据
    """
    df_merge = df_dq.merge(df_a, on=['year', 'month', 'day', 'SecuCode', 'TradingDay'])
    df_merge = df_merge.merge(df_f, on=['SecuCode', 'year'])
    df_td = ReadTradingDay(para)
    df_merge = df_merge.merge(df_td, on=['year', 'month', 'day', 'TradingDay'])
    col = df_merge.pop('TimeIndex')
    df_merge.insert(loc=5, column='TimeIndex', value=col)
    col = df_merge.pop('REV')
    df_merge.insert(loc=6, column='REV', value=col)

    return df_merge


# 3. 数据处理
# - 3.1 提取因子名称
def FactorName(df):
    a_factor_name = df.columns
    a_factor_name = list(a_factor_name)
    a_factor_name = a_factor_name[7:]

    # for a_factor in a_factor_name:
    # print(a_factor,type(a_factor))

    return a_factor_name


# - 3.2 去除极端值
def ExtremeValueHandling(para, df, afactor_name):
    """
    df：DataFrame数据
    factor：为需要去极值的列名称，str
    n： 判断极值上下边界的常数， int
    """
    #
    n = para.n_extreme
    # 提取该列的数据
    ls_raw = np.array(df[afactor_name].values)
    # 删除缺失值
    ls_raw = ls_raw[~np.isnan(ls_raw)]
    # 排序 axis=0，按列排列
    ls_raw.sort(axis=0)
    # 获取中位数
    dm = np.median(ls_raw)

    # 计算离差值
    ls_deviation = abs(ls_raw - dm)
    # 排序
    ls_deviation.sort(axis=0)
    # 获取离差中位数
    dmad = np.median(ls_deviation)

    # 将大于中位数n倍离差中位数的值赋为NaN
    df.loc[df[afactor_name] >= dm + n * dmad, afactor_name] = None
    # 将小于中位数n倍离差中位数的值赋为NaN
    df.loc[df[afactor_name] <= dm - n * dmad, afactor_name] = None

    return df


# - 3.3 缺失值处理
def MissingDataHandling(df, afactor_name):
    """
    df：DataFrame数据
    factor：为需要去缺失值的列名称，str
    用每列的中位数填补缺失值
    """

    # 提取该列的数据
    ls_raw = np.array(df[afactor_name].values)
    # 删除缺失值
    ls_raw = ls_raw[~np.isnan(ls_raw)]
    # 排序 axis=0，按列排列
    ls_raw.sort(axis=0)
    # 获取中位数
    dm = np.median(ls_raw)
    # 填补缺失值
    df[afactor_name] = df[afactor_name].fillna(dm)

    return df


# - 3.4 标准化处理
def Standardize(df, afactor_name):
    """
    df：DataFrame数据
    factor：为需要去缺失值的列名称，str
    """
    df[afactor_name] = (df[afactor_name] - df[afactor_name].mean()) / df[afactor_name].std()

    return df


# - 3.5 生产完整的因子数据
def GenerateFactorData(para):
    print('# READING Model')
    financial_factor = Financial_Factor(ReadData(para.path_financial, para.begin_year, para.end_year))
    daily_quote_factor = Daily_Quote_Factor(ReadData(para.path_dailyquote, para.begin_year, para.end_year))
    analyst_factor = Analyst_Factor(ReadData(para.path_analyst, para.begin_year, para.end_year))
    df_factor = MergeFactors(para, financial_factor, analyst_factor, daily_quote_factor)
    afactor_name = FactorName(df_factor)

    for afactor in afactor_name:
        # print(afactor)
        df_factor = ExtremeValueHandling(para, df_factor, afactor)
        df_factor = MissingDataHandling(df_factor, afactor)
        df_factor = Standardize(df_factor, afactor)

    df_factor = df_factor.sort_values(by=['TradingDay', 'SecuCode'])

    # -- 更新参数
    para.begin_timeindex = df_factor['TimeIndex'].min()
    para.end_timeindex = df_factor['TimeIndex'].max()
    para.time_in_sample = range(para.begin_timeindex,
                                para.begin_timeindex+int((para.end_timeindex - para.begin_timeindex) * para.in_sample_rate) + 1)
    para.time_test = range(para.begin_timeindex+int((para.end_timeindex - para.begin_timeindex) * para.in_sample_rate) + 1,
                           para.end_timeindex + 1)

    return df_factor


# 4. 设置模型
# -- 4.1 筛选出收益最高和最低的n只股票
def label_data(data):
    # -- label data
    data = data.copy()
    data['REV_bin'] = np.nan

    # -- sort by return
    data = data.sort_values(by='REV', ascending=False)

    # -- decide the amount of stocks selected
    n_stock_select = np.multiply(para.percent_select, data.shape[0])
    n_stock_select = np.around(n_stock_select).astype(int)

    # -- assign 1 or 0
    data.iloc[0:n_stock_select[0], -1] = 1
    data.iloc[-n_stock_select[1]:, -1] = 0

    # -- remove other stocks
    data = data.dropna(axis=0)

    return data


para = Para()
df = GenerateFactorData(para)

print('# TESTING Model')
print('Total TimeIndex:[', str(para.begin_timeindex), ',', str(para.end_timeindex), ']')
print('Test TimeIndex(in-sample):[', str(para.begin_timeindex), ',',
      str(para.begin_timeindex+int((para.end_timeindex - para.begin_timeindex) * para.in_sample_rate)), ']')
print('Predict TimeIndex(out-sample):[',
      str(int(para.begin_timeindex+(para.end_timeindex - para.begin_timeindex) * para.in_sample_rate) + 1), ',', str(para.end_timeindex),
      ']')

# - 4.2 生成样本内数据集
# -- generate in-sample data
data_in_sample = pd.DataFrame()
para.n_stock = 0
for i_time in para.time_in_sample:
    data_curr_day = df[df['TimeIndex'] == i_time]
    if data_curr_day.shape[0] > para.n_stock:
        para.n_stock = data_curr_day.shape[0]
    data_curr_day = label_data(data_curr_day)
    data_in_sample = pd.concat((data_in_sample, data_curr_day), axis=0)

# - 4.3 划分标签集和特征集
# -- Divide label values from feature values
# - 特征集
X_in_sample = data_in_sample.loc[:, 'Size':'NPT']

# - 标签集
# -- classification
if para.method in ['LOGI', 'XGBOOST-C']:
    y_in_sample = data_in_sample.loc[:, 'REV_bin']

# -- regression
if para.method in ['LR']:
    y_in_sample = data_in_sample.loc[:, 'REV']

# - 4.4 划分训练集和验证集
# -- generate train and cv data
from sklearn.model_selection import train_test_split

if para.percent_cv > 0:
    X_train, X_cv, y_train, y_cv = train_test_split(X_in_sample, y_in_sample, test_size=para.percent_cv,
                                                    random_state=para.seed)
else:
    X_train, y_train = X_in_sample.copy(), y_in_sample.copy()

# - 4.5 设置模型
# -- set model
# -- logistic regression
if para.method == 'LOGI':
    from sklearn import linear_model

    model = linear_model.LogisticRegression(C=para.logi_c)

# -- XGBoost Classifier
if para.method == 'XGBOOST-C':
    from xgboost import XGBClassifier

    model = XGBClassifier(random_state=para.seed,
                          n_estimators=para.xgbc_n_estimators,
                          learning_rate=para.xgbc_learning_rate,
                          subsample=para.xgbc_subsample_C,
                          max_depth=para.xgbc_max_depth)

# -- linear regression
if para.method == 'LR':
    from sklearn import linear_model

    model = linear_model.LinearRegression(fit_intercept=True)

# - 4.6 训练模型，交叉验证
# -- train model, and perform cross validation
# - train model
# -- classification
if para.method in ['LOGI', 'XGBOOST-C']:
    model.fit(X_train, y_train)
    # -- y_pred: binary format; y_score: continious format
    y_pred_train = model.predict(X_train)
    y_score_train = model.predict_proba(X_train)[:, 1]

    if para.percent_cv > 0:
        y_pred_cv = model.predict(X_cv)
        y_score_cv = model.predict_proba(X_cv)[:, 1]

# -- regression
if para.method in ['LR']:
    model.fit(X_train, y_train)
    y_score_train = model.predict(X_train)

    if para.percent_cv > 0:
        y_score_cv = model.predict(X_cv)

# - cross validation
if para.percent_cv > 0:
    print('1.CROSS-VALIDATION:')
    if para.evaluation_metrics == 'cross_val_score':
        from sklearn.model_selection import cross_val_score

        scores = cross_val_score(model, X_train, y_train, cv=5)
        print(scores)
    if para.evaluation_metrics == 'cross_validate':
        from sklearn.model_selection import cross_validate
        from sklearn.metrics import recall_score

        scoring = ['precision_macro', 'recall_macro']
        scores = cross_validate(model, X_train, y_train, scoring=scoring, cv=5)
        print(sorted(scores.keys()))
        print(scores['test_recall_macro'])

    print('2.INTERNAL DEPENDENCE:')
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import median_absolute_error
    from sklearn.metrics import r2_score

    mse = mean_squared_error(y_cv, y_score_cv)
    print("Mean Squared Error(MSE) = %.6f" % mse)
    mae = median_absolute_error(y_cv, y_score_cv)
    print("Median Absolute Error(MAE) = %.6f" % mae)
    r2 = r2_score(y_cv, y_score_cv)
    print("R^2 score = %.6f" % r2)

# - 4.7 样本外预测
# -- predict
y_true_test = pd.DataFrame([np.nan] * np.zeros((para.n_stock + 1000, para.time_test[-1])))
y_pred_test = pd.DataFrame([np.nan] * np.zeros((para.n_stock + 1000, para.time_test[-1])))
y_score_test = pd.DataFrame([np.nan] * np.zeros((para.n_stock + 1000, para.time_test[-1])))

for i_time in para.time_test:
    data_curr_day = df[df['TimeIndex'] == i_time]
    data_curr_day.reset_index(inplace=True, drop=True)
    # -- generate X
    X_curr_day = data_curr_day.loc[:, 'Size':'NPT']

    # -- predict and get predicted probability
    # -- classification
    if para.method in ['LOGI', 'XGBOOST-C']:
        y_pred_curr_day = model.predict(X_curr_day)
        y_score_curr_day = model.predict_proba(X_curr_day)[:, 1]

    # -- linear regression
    if para.method in ['LR', ]:
        y_score_curr_day = model.predict(X_curr_day)

    # -- store real and predicted return
    y_true_test.iloc[data_curr_day.index, i_time - 1] = data_curr_day['REV'][data_curr_day.index]
    if para.method in ['LOGI', 'XGBOOST-C']:
        y_pred_test.iloc[data_curr_day.index, i_time - 1] = y_pred_curr_day
    y_score_test.iloc[data_curr_day.index, i_time - 1] = y_score_curr_day

# - 4.8 模型评价
# -- evaluate
print('3.MODOLE EVALUATION:')
if para.method in ['LOGI', 'XGBOOST-C']:
    output_acc_auc = pd.DataFrame([np.nan] * np.zeros((para.time_test[-1], 6)))

    from sklearn import metrics

    print('training set, accuracy = %.2f' % metrics.accuracy_score(y_train, y_pred_train))
    print('training set, AUC = %.2f' % metrics.roc_auc_score(y_train, y_score_train))
    output_acc_auc.iloc[para.time_in_sample[-1] - 1, 0] = metrics.accuracy_score(y_train, y_pred_train)
    output_acc_auc.iloc[para.time_in_sample[-1] - 1, 3] = metrics.roc_auc_score(y_train, y_score_train)

    if para.percent_cv > 0:
        print('cv set, accuracy = %.2f' % metrics.accuracy_score(y_cv, y_pred_cv))
        print('cv set, AUC = %.2f' % metrics.roc_auc_score(y_cv, y_score_cv))
        output_acc_auc.iloc[para.time_in_sample[-1] - 1, 1] = metrics.accuracy_score(y_cv, y_pred_cv)
        output_acc_auc.iloc[para.time_in_sample[-1] - 1, 4] = metrics.roc_auc_score(y_cv, y_score_cv)

    for i_time in para.time_test:
        # -- 4 types of y
        # -- y_true_*: true continious
        # -- y_*: true binary
        # -- y_pred_*: predicted binary
        # -- y_score_*: predicted continious
        y_true_curr_day = pd.DataFrame({'REV': y_true_test.iloc[:, i_time - 1]})
        y_pred_curr_day = y_pred_test.iloc[:, i_time - 1]
        y_score_curr_day = y_score_test.iloc[:, i_time - 1]

        # -- remove nan
        y_true_curr_day = y_true_curr_day.dropna(axis=0)

        # -- label data
        y_curr_day = label_data(y_true_curr_day)['REV_bin']

        # -- only select best and worst 30% data
        y_pred_curr_day = y_pred_curr_day[y_curr_day.index]
        y_score_curr_day = y_score_curr_day[y_curr_day.index]

        print('testing set, time %d, accuracy = %.2f' % (i_time, metrics.accuracy_score(y_curr_day, y_pred_curr_day)))
        print('testing set, time %d, AUC = %.2f' % (i_time, metrics.roc_auc_score(y_curr_day, y_score_curr_day)))
        output_acc_auc.iloc[i_time - 1, 2] = metrics.accuracy_score(y_curr_day, y_pred_curr_day)
        output_acc_auc.iloc[i_time - 1, 5] = metrics.roc_auc_score(y_curr_day, y_score_curr_day)

if para.method in ['LR']:
    output_ic = pd.DataFrame([np.nan] * np.zeros((para.time_test[-1], 3)))

    y_train.index = range(len(y_train))
    y_score_train = pd.Series(y_score_train)
    print('training set, ic = %.2f' % y_train.corr(y_score_train))
    output_ic.iloc[para.time_in_sample[-1] - 1, 0] = y_train.corr(y_score_train)

    if para.percent_cv > 0:
        y_cv.index = range(len(y_cv))
        y_score_cv = pd.Series(y_score_cv)
        print('cv set, ic = %.2f' % y_cv.corr(y_score_cv))
        output_ic.iloc[para.time_in_sample[-1] - 1, 1] = y_cv.corr(y_score_cv)

    for i_time in para.time_test:
        y_true_curr_day = y_true_test.iloc[:, i_time - 1]
        y_score_curr_day = y_score_test.iloc[:, i_time - 1]
        print('testing set, time %d, ic = %.2f' % (i_time, y_true_curr_day.corr(y_score_curr_day)))
        output_ic.iloc[i_time - 1, 2] = y_true_curr_day.corr(y_score_curr_day)

# 5. 简易回测
# -- simple strategy, select 50 stocks every month, equally weighted
para.n_stock_select = 50
strategy = pd.DataFrame({'REV': [0] * (para.time_test[-1] + 1), 'value': [1] * (para.time_test[-1] + 1)})
print('4.Back Testing:')
for i_time in para.time_test:
    # -- get real and predicted return
    y_true_curr_day = y_true_test.iloc[:, i_time - 1]
    y_score_curr_day = y_score_test.iloc[:, i_time - 1]

    # -- sort predicted return, and choose the best 50
    y_score_curr_day = y_score_curr_day.sort_values(ascending=False)
    index_select = y_score_curr_day[0:para.n_stock_select].index

    # -- take the average return as the return of next month
    strategy.loc[i_time - 1, 'REV'] = np.mean(y_true_curr_day[index_select])

# -- compute the compund value of the strategy
strategy['value'] = (strategy['REV'] + 1).cumprod()

# -- plot the value
import matplotlib.pyplot as plt

plt.xlabel('TimeIndex')
plt.ylabel('Daily excess return')
plt.plot(para.time_test, strategy.loc[para.time_test, 'REV'], 'r-')
plt.show()

# -- evaluation
ann_excess_return = np.mean(strategy.loc[para.time_test, 'REV']) * 365
ann_excess_vol = np.std(strategy.loc[para.time_test, 'REV']) * np.sqrt(365)
info_ratio = ann_excess_return / ann_excess_vol

print('annual excess return = %.2f' % ann_excess_return)
print('annual excess volatility = %.2f' % ann_excess_vol)
print('information ratio = %.2f' % info_ratio)
```

